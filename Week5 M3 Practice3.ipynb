{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d39627f0-de41-430a-875f-082568067aec",
   "metadata": {},
   "source": [
    "# Pyspark + Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "280f93b1-0f70-49bd-8652-e5594b391f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6521aa62-9791-4e9d-acc3-0ae6fc4578af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SparkSession:\n",
    "spark = SparkSession.builder.master('local').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ab4129a-e6a0-4408-905e-0e6c8a114e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the data and save as Dataframe\n",
    "spark_df = spark.read.csv('forestfires.csv', header='true', inferSchema='true')\n",
    "type(spark_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1a8c85e-cfea-46ce-8393-7f1e055483eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[X: int, Y: int, month: string, day: string, FFMC: double, DMC: double, DC: double, ISI: double, temp: double, RH: int, wind: double, rain: double, area: double]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f705361-5b76-4ecd-8882-2ac87f418671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X',\n",
       " 'Y',\n",
       " 'month',\n",
       " 'day',\n",
       " 'FFMC',\n",
       " 'DMC',\n",
       " 'DC',\n",
       " 'ISI',\n",
       " 'temp',\n",
       " 'RH',\n",
       " 'wind',\n",
       " 'rain',\n",
       " 'area']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "579a155e-7e00-4b54-b73a-3f450dc56c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+\n",
      "|month| RH|rain|\n",
      "+-----+---+----+\n",
      "|  mar| 51| 0.0|\n",
      "|  oct| 33| 0.0|\n",
      "|  oct| 33| 0.0|\n",
      "|  mar| 97| 0.2|\n",
      "|  mar| 99| 0.0|\n",
      "|  aug| 29| 0.0|\n",
      "|  aug| 27| 0.0|\n",
      "|  aug| 86| 0.0|\n",
      "|  sep| 63| 0.0|\n",
      "|  sep| 40| 0.0|\n",
      "|  sep| 51| 0.0|\n",
      "|  sep| 38| 0.0|\n",
      "|  aug| 72| 0.0|\n",
      "|  sep| 42| 0.0|\n",
      "|  sep| 21| 0.0|\n",
      "|  sep| 44| 0.0|\n",
      "|  mar| 27| 0.0|\n",
      "|  oct| 47| 0.0|\n",
      "|  mar| 35| 0.0|\n",
      "|  apr| 44| 0.0|\n",
      "+-----+---+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df[['month','RH','rain']].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea679e71-cee8-4679-baed-eb31042ae054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[rain: double]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting one column is different to pandas\n",
    "spark_df.select('rain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cc6b207-583c-4e44-a8da-8d5b505008f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('X', 'int'),\n",
       " ('Y', 'int'),\n",
       " ('month', 'string'),\n",
       " ('day', 'string'),\n",
       " ('FFMC', 'double'),\n",
       " ('DMC', 'double'),\n",
       " ('DC', 'double'),\n",
       " ('ISI', 'double'),\n",
       " ('temp', 'double'),\n",
       " ('RH', 'int'),\n",
       " ('wind', 'double'),\n",
       " ('rain', 'double'),\n",
       " ('area', 'double')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "227c7d07-abd9-45a8-83ab-a82f181e223d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month: string, avg(area): double]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlation\n",
    "# Put all the month together (groupby), then give me the mean of the column area \n",
    "spark_df_months = spark_df.groupBy('month').agg({'area': 'mean'})\n",
    "spark_df_months"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848d3d18-cc82-4d04-a6b3-765e0c3a1de8",
   "metadata": {},
   "source": [
    "OBS! Notice how the grouped DataFrame is not returned when you call the aggregation method. Remember, this is still Spark! The transformations and actions are kept separate so that it is easier to manage large quantities of data. You can perform the transformation by calling .collect():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9b6f425-b93e-498f-9f5b-74eb1132d622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(month='jun', avg(area)=5.841176470588234),\n",
       " Row(month='aug', avg(area)=12.489076086956521),\n",
       " Row(month='may', avg(area)=19.24),\n",
       " Row(month='feb', avg(area)=6.275),\n",
       " Row(month='sep', avg(area)=17.942616279069753),\n",
       " Row(month='mar', avg(area)=4.356666666666667),\n",
       " Row(month='oct', avg(area)=6.638),\n",
       " Row(month='jul', avg(area)=14.3696875),\n",
       " Row(month='nov', avg(area)=0.0),\n",
       " Row(month='apr', avg(area)=8.891111111111112),\n",
       " Row(month='dec', avg(area)=13.33),\n",
       " Row(month='jan', avg(area)=0.0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df_months.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd269aa6-a902-4a04-9c4c-2f0d73283ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month: string, avg(area): double]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df_months"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d255a413-9d69-46aa-8d42-f4f1b01285f6",
   "metadata": {},
   "source": [
    "## Boolean Masking\n",
    "\n",
    "Boolean masking also works with PySpark DataFrames just like Pandas DataFrames, the only difference being that the .filter() method is used in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3bd5477e-1487-4e77-8e85-025b55ea4028",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_rain = spark_df.filter(spark_df['rain'] == 0.0)\n",
    "some_rain = spark_df.filter(spark_df['rain'] > 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b40e83-2290-406c-a960-1fd78384ae94",
   "metadata": {},
   "source": [
    "To perform calculations to find the mean of a column, we'll have to import functions from pyspark.sql. As always, to read more about them, check out the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c30c4431-59d9-4378-b354-0a14d337cbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         avg(area)|\n",
      "+------------------+\n",
      "|13.023693516699408|\n",
      "+------------------+\n",
      "\n",
      "no rain fire area:  None \n",
      "\n",
      "+---------+\n",
      "|avg(area)|\n",
      "+---------+\n",
      "|  1.62375|\n",
      "+---------+\n",
      "\n",
      "some rain fire area:  None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "print('no rain fire area: ', no_rain.select(mean('area')).show(),'\\n')\n",
    "\n",
    "print('some rain fire area: ', some_rain.select(mean('area')).show(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ec047ef-ee76-40f0-98c6-bb8b83bf2942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         avg(area)|\n",
      "+------------------+\n",
      "|13.023693516699408|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_rain.select(mean('area')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea5ab24-3f5f-48eb-8388-67b26685ead9",
   "metadata": {},
   "source": [
    "Obtain data from only the summer months in Portugal (June, July, and August). We can also do the same for the winter months in Portugal (December, January, February)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45eddba7-da6b-4491-8425-0df1888a8102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         avg(area)|\n",
      "+------------------+\n",
      "|12.262317596566525|\n",
      "+------------------+\n",
      "\n",
      "summer months fire area None\n",
      "+-----------------+\n",
      "|        avg(area)|\n",
      "+-----------------+\n",
      "|7.918387096774193|\n",
      "+-----------------+\n",
      "\n",
      "winter months fire areas None\n"
     ]
    }
   ],
   "source": [
    "summer_months = spark_df.filter(spark_df['month'].isin(['jun','jul','aug']))\n",
    "winter_months = spark_df.filter(spark_df['month'].isin(['dec','jan','feb']))\n",
    "\n",
    "print('summer months fire area', summer_months.select(mean('area')).show())\n",
    "print('winter months fire areas', winter_months.select(mean('area')).show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24170511-00d5-4161-84bc-64ae976885cc",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce865ed6-1c23-4ef0-b823-8d07406917c0",
   "metadata": {},
   "source": [
    "SELECT CASE\n",
    "       WHEN EDUCATION = '0' THEN 'Other'\n",
    "       WHEN EDUCATION = '5' THEN 'Other'\n",
    "       WHEN EDUCATION = '6' THEN 'Other'\n",
    "       ELSE EDUCATION\n",
    "       END AS EDUCATION\n",
    "  FROM credit_card_default;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e928894-20fb-4d5e-bc4f-16f28bf6d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fbf5ab5-0308-4753-96fb-d3b0d6aac653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I define which month is in every season\n",
    "\n",
    "df_months_binned = spark_df.withColumn('month',\n",
    "                                         F.when(spark_df['month'] == 'jun', 'Summer') \\\n",
    "                                         .when(spark_df['month'] == 'jul', 'Summer') \\\n",
    "                                         .when(spark_df['month'] == 'aug', 'Summer') \\\n",
    "                                         .when(spark_df['month'] == 'jan', 'Winter') \\\n",
    "                                         .when(spark_df['month'] == 'feb', 'Winter') \\\n",
    "                                         .when(spark_df['month'] == 'dec', 'Winter') \\\n",
    "                                         .otherwise('Spring/Fall')\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64284434-d8ff-4c66-81cb-ec9eb13a4245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|      month|         avg(area)|\n",
      "+-----------+------------------+\n",
      "|     Summer|12.262317596566525|\n",
      "|Spring/Fall|13.989960474308296|\n",
      "|     Winter| 7.918387096774193|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select both 'month' and 'area' before grouping and aggregating\n",
    "\n",
    "result = df_months_binned.select('month', 'area').groupBy('month').agg({'area': 'mean'}).distinct()\n",
    "result.show() # To display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229e2a51-431d-44f5-b849-31e186be3d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
